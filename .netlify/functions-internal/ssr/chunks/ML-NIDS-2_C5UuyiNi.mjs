const id = "ML-NIDS-2.md";
						const collection = "projects";
						const slug = "ml-nids-2";
						const body = "_If you haven’t yet, check [part I](/src/assets/projects/ml-nids-1) of this project where we performed some data pre-processing, exploratory data analysis, and feature engineering (PCA) on the original dataset._\n\nAs a reminder, the dataset we’re using can be found [here](https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection). We are also training the machine learning models on the 10 principal components produced by our feature engineering approach, except for the DNN which will be using the original features.\n\nWe will start by defining a helpful function to print the error metrics of the ML models we will train.\n\n```python\ndef print_error_metrics(y_test, y_pred):  \n    acc = metrics.accuracy_score(y_test, y_pred)  \n    prc = metrics.precision_score(y_test, y_pred)  \n    f1 = metrics.f1_score(y_test, y_pred)  \n    print('Accuracy: {:.5f}'.format(acc))  \n    print('Precision: {:.5f}'.format(prc))  \n    print('F1 Score: {:.5f}'.format(f1))\n```\n\n## Logistic Regression Models\n\nWe will use an unregularized logistic regression model to fit on the training data.\n\n```python\nregressor = LogisticRegression(max_iter=100, penalty='none')  \nregressor.fit(X_train_PCA, y_train)  \ny_pred = regressor.predict(X_test_PCA)  \nprint_error_metrics(y_test, y_pred)\n```\n\nWe obtain the following metrics on this model: Accuracy: 0.95773, Precision: 0.96615, and F1 Score: 0.95379. While promising, we regularize this version by using an elastic net regularization with an L1 ratio of 0.5. This will be helpful in avoiding overfitting which can make the model more generalizable to unseen data.\n\n```python\nregressor = LogisticRegression(max_iter=400, solver='saga', penalty='elasticnet', l1_ratio=0.5)  \nregressor.fit(X_train_PCA, y_train)  \ny_pred = regressor.predict(X_test_PCA)  \nprint_error_metrics(y_test, y_pred)\n```\n\n## Decision Trees\n\n```python\ndtree = DecisionTreeClassifier(max_depth=None)  \ndtree.fit(X_train_PCA, y_train)  \nprint(\"Decision tree maximum depth:\", dtree.tree_.max_depth)  \ny_pred = dtree.predict(X_test_PCA)  \nprint_error_metrics(y_test, y_pred)\n```\n\nAfter training a decision tree classifier, we obtain the following metrics: Accuracy: 0.99147, Precision: 0.98974, and F1 Score: 0.99080.\n\nWhile the results are promising, after looking into the decision tree produced, we realize that its maximum depth is 19. This implies that it might be overfitting on the training data: some decisions aren’t obtained until 19 separate splits are made on one of the 10 principal components. A zoomed out snapshot of the decision tree (as well as the code that produces it) is provided below. Note that the code changes the default color of the decision tree to paint the normal class in green and the anomalous class in red.\n\n```python\nfrom matplotlib.colors import to_rgb  \n  \nfeatures = ['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10']  \nfig = plt.figure(figsize=(100,90))  \nclass_colors=['green', 'red']  \nartists = plot_tree(dtree, feature_names=features, class_names=['Normal', 'Anomaly'], filled=True, rounded=True, fontsize=10)  \nfor artist, impurity, value in zip(artists, dtree.tree_.impurity, dtree.tree_.value):  \n    r, g, b = to_rgb(class_colors[np.argmax(value)])  \n    f = impurity * 2  \n    artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b))  \n    artist.get_bbox_patch().set_edgecolor('black')  \nfig.savefig('decision_tree_1.png')\n```\n![Original decision tree](/src/assets/ml_nid/decision_tree_1.png)\nAs you can see from the tree structure, this is an overly complex models considering it’s trained on 10 features. To regularize this model, we manually set the maximum depth of the tree to 5 in the _DecisionTreeClassifier_ class parameter. As a result, we obtain the following metrics: Accuracy: 0.97658, Precision: 0.98300, and F1 Score: 0.97450. The decision tree produced is found below.\n\n![Decision tree model with maximum depth of 5](/src/assets/ml_nid/decision_tree_2.png)\n\nNote that the red leaves indicate a majority of malicious samples (so it will be predicted as malicious) and the green leaves have a majority of benign samples (predicted as benign). The more transparent a leaf is, the higher its entropy (the model will predict the majority class label but will be unsure about it). Even though we significantly reduced the tree’s depth from 19 to 5, its accuracy and precision barely dropped. This makes us more confident in recommending this version of the regularized model as opposed to the overfit one.\n\n## Deep Neural Network\n\nWe start by specifying the architecture of the model. The neural network passes the original 118 features to a hidden layer of 20 nodes which then passes them to another hidden layer of 10 nodes which finally sends them to an output node. Note that the ReLU activation function is used (to avoid the problem of vanishing gradients) except for the final layer which uses the sigmoid activation function to output probability values.\n\n```python\nclass SimpleNN(torch.nn.Module):  \n    def __init__(self):  \n        super(SimpleNN, self).__init__()  \n        self.fc1 = torch.nn.Linear(118, 20)  \n        self.relu1 = torch.nn.ReLU()  \n        self.fc2 = torch.nn.Linear(20, 10)  \n        self.relu2 = torch.nn.ReLU()  \n        self.fc3 = torch.nn.Linear(10, 1)  \n        self.sigm = torch.nn.Sigmoid()  \n  \n    def forward(self, x):  \n        x = self.fc1(x)  \n        x = self.relu1(x)  \n        x = self.fc2(x)  \n        x = self.relu2(x)  \n        x = self.fc3(x)  \n        x = self.sigm(x)  \n        return x\n```\n\nWe will use the binary cross entropy loss function as well as the Adam optimizer to train this model on the training data.\n\n```python\nmodel = SimpleNN()  \ncriterion = torch.nn.BCELoss()  \noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \ndataset = TensorDataset(torch.from_numpy(X_train_norm).type(torch.float), torch.from_numpy(y_train.to_numpy()).type(torch.float))  \ntrain_loader = DataLoader(dataset, batch_size=64, shuffle=True)  \nX_test_DNN = torch.from_numpy(X_test_norm).type(torch.float)  \ny_test_DNN = torch.from_numpy(y_test.to_numpy()).type(torch.float)  \n  \nepochs = 20  \nloss_value = 0.0  \ntrain_loss = []  \ntest_loss = []  \nfor epoch in range(epochs):  \n    model.train()  \n    i = 0  \n    for _batch_idx, (features, labels) in enumerate(tqdm(train_loader)):  \n        i += 1  \n        optimizer.zero_grad()  \n        outputs = model(features).squeeze()  \n        loss = criterion(outputs, labels)  \n        loss.backward()  \n        optimizer.step()  \n  \n        loss_value += loss.item()  \n    train_loss.append(loss_value/i)  \n    # Testing  \n    model.eval()  \n    with torch.inference_mode():  \n        test_pred = model(X_test_DNN).squeeze()  \n        test_loss_val = criterion(test_pred, y_test_DNN).item()  \n        test_loss.append(test_loss_val)  \n        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss_value/i:.10f}, Test Loss: {test_loss_val:.10f}')  \n    loss_value = 0.0  \nprint(train_loss)\n```\n\nAfter 20 epochs, we obtain the following train and test loss curves:\n\n![DNN training for 20 epochs](/src/assets/ml_nid/loss1.png)\n\nConsidering that the testing loss fluctuates and increases slightly beyond the fifth epoch while the training loss is decreasing, we decide to employ early stopping to stop training the model at epoch 5 (by changing our _epoch_ variable to 5 in the code above). This results in the following learning curve:\n\n![DNN model training with 5 epochs](/src/assets/ml_nid/loss2.png)\n\nTo evaluate our model, we run the code below:\n\n```python\nmodel.eval()  \nwith torch.inference_mode():  \n    test_pred = model(X_test_DNN).squeeze()  \n    test_loss = criterion(test_pred, y_test_DNN)  \n    y_pred = []  \n    for pred in test_pred:  \n        if pred>0.5: y_pred.append(1)  \n        else: y_pred.append(0)  \n    print_error_metrics(y_test_DNN, y_pred)\n```\n\nWe obtain the following metrics: Accuracy: 0.99484, Precision: 0.99656, and F1 Score: 0.99442.\n\n## Conclusion\n\nIn conclusion, it is clear how data pre-processing and feature engineering can be of incredible help in the ML workflow. As for the specific ML models, we recommend the use of decision trees (regularized with a maximum depth of 5) for network intrusion detection as captured by the dataset we used. This model provides high accuracy for intrusion detection while being regularized against overfitting and being explainable in nature as captured by its decision nodes (as opposed to the unexplainable nature of the deep neural network model we developed).";
						const data = {title:"Machine Learning for Network Intrusion Detection — Part II: Machine Learning Training",description:"",pubDate:"Dec 17 2023",heroImage:"src/assets/ml_nid/hero2.jpg",tags:["Machine Learning","Cybersecurity","Python","PyTorch","Scikit-learn"]};
						const _internal = {
							type: 'content',
							filePath: "/Users/elirizk/Desktop/Personal Website/public_site/src/content/projects/ML-NIDS-2.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
