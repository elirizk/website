import { f as createComponent, r as renderTemplate, m as maybeRenderHead, u as unescapeHTML } from './astro_BaztoWCL.mjs';
import 'kleur/colors';
import 'clsx';

const html = "<p>Traditional intrusion detection systems have used the notion of allow or block lists to only permit harmless operations or to detect known malicious activities, respectively. This requires a regularly updated configuration file listing the user operations to allow or block, e.g. known malicious IP addresses, outside worktime logins, etc. While this type of network intrusion detection system requires constant modification and a considerable initial investment to produce the original configuration, a novel approach to intrusion detection has emerged: the use of machine learning models to automate the task of detecting potential intruders. As such, in this post, we will develop a machine learning model to recognize hidden patterns in anomalous network activity. To this end, we will perform some data pre-processing on the dataset we obtain, explore various machine learning models, and recommend the best model according to their performance and other metrics. The dataset used for the rest of our implementation can be found <a href=\"https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection\">here</a>.</p>\n<h2 id=\"import-libraries-and-set-display-configuration\">Import libraries and set display configuration</h2>\n<p>Before processing our dataset, we will have to import the necessary libraries to use their helpful commands.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.utils.data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DataLoader, TensorDataset  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torchviz </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> make_dot  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> mpl_toolkits.mplot3d </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Axes3D  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.model_selection </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> train_test_split  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> metrics, preprocessing  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.decomposition </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> PCA</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.linear_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LogisticRegression  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.tree </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DecisionTreeClassifier, plot_tree  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.ensemble </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RandomForestClassifier  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tqdm </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tqdm  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"></span></code></pre>\n<p>I also like setting the display configuration of the pandas module to render all columns, rows, or sequences (as opposed to setting a threshold on its maximum value). This can be done with the following code:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">pd.set_option(</span><span style=\"color:#9ECBFF\">'display.max_columns'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">pd.set_option(</span><span style=\"color:#9ECBFF\">'display.max_rows'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">pd.set_option(</span><span style=\"color:#9ECBFF\">'display.max_seq_item'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<h2 id=\"exploratory-data-analysis\">Exploratory Data Analysis</h2>\n<p>We will start by examining the columns available in the dataset.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df.columns  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pd.read_csv(</span><span style=\"color:#9ECBFF\">'Train_data.csv'</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.columns</span></span>\n<span class=\"line\"></span></code></pre>\n<p>We obtain the following list of features and class labels (note that the column <em>class</em> contains the label of the entry: Normal or Anomaly). We also note that the columns: <em>protocol_type</em>, <em>service</em> and <em>flag</em> are categorical while the rest are numerical. The total number of columns for this dataset is 43 columns.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'duration'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'protocol_type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'service'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'flag'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'src_bytes'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_bytes'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'land'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'wrong_fragment'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'urgent'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'hot'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'num_failed_logins'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'logged_in'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'num_compromised'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'root_shell'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'su_attempted'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'num_root'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'num_file_creations'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'num_shells'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'num_access_files'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'num_outbound_cmds'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'is_host_login'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'is_guest_login'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'count'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'srv_count'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'serror_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'srv_serror_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'rerror_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'srv_rerror_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'same_srv_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'diff_srv_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'srv_diff_host_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dst_host_count'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_host_srv_count'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dst_host_same_srv_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_host_diff_srv_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dst_host_same_src_port_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_host_srv_diff_host_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dst_host_serror_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_host_srv_serror_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'dst_host_rerror_rate'</span><span style=\"color:#E1E4E8\">,  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'dst_host_srv_rerror_rate'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span></code></pre>\n<p>To check for class imbalances, we plot the number of entries that are labelled as suspicious versus normal. This can safeguard against potential biases in the model being trained (adjusting for class imbalances).</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">sns.countplot(</span><span style=\"color:#FFAB70\">x</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">palette</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'green'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'red'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">hue</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/class_count.png\" alt=\"&#x27;Class Count\">\nAs we can see both classes are almost equally represented in the dataset. Hence, we don’t need to remove any instances of the majority class (undersampling).</p>\n<h2 id=\"data-correlation\">Data Correlation</h2>\n<p>We now go ahead with transforming the class label column to a binary value (0 for normal and 1 for anomaly) and providing the Pearson correlation matrix of the dataset (excluding the categorical feature columns).</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">categorical_columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'protocol_type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'service'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'flag'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> x: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> x</span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\">\"normal\"</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">40</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sns.heatmap(df.drop(categorical_columns, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">).corr(), </span><span style=\"color:#FFAB70\">annot</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/corr_heatmap.png\" alt=\"Correlation matrix with the 39 numerical features and the class label (last column/row)\"></p>\n<p>As can be seen, most features aren’t highly correlated together (colored in light purple for a value close to 0) while some feature pairs have either high positive (close to 1) or negative (close to -1) correlation (colored in white or dark purple respectively). For the class label column (last column and last index), only 9 of the 42 numerical features are highly correlated (with an absolute value greater than 0.5). However, this doesn’t mean that we can easily disregard the features with correlation close to 0. This is because the Pearson correlation value assumes a linear relationship between the two variable, which is an overly simplistic assumption in our case (it might be the case that the interplay of different features is highly correlated with the class label or that the feature is highly correlated in a nonlinear fashion). For example, we decided to plot the variation of the same server rate variable against the destination host name server rate, producing the following scatter plot. Note that for the rest of this post, entries labeled as normal are visualized in green while those labeled anomalous are labeled in red.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">colors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'green'</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#9ECBFF\">'red'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[</span><span style=\"color:#9ECBFF\">'dst_host_same_srv_rate'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[</span><span style=\"color:#9ECBFF\">'same_srv_rate'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.scatter(x,y, </span><span style=\"color:#FFAB70\">c</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">labels, </span><span style=\"color:#FFAB70\">cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">matplotlib.colors.ListedColormap(colors), </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">s</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">40</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.xlabel(</span><span style=\"color:#9ECBFF\">\"Same Server Rate\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.ylabel(</span><span style=\"color:#9ECBFF\">\"Destination Host Name Server Rate\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/same_srv_rate_vs_dst_host_same_srv_rate.png\" alt=\"Scatter plot of the same server rate versus the destination host name server rate\"></p>\n<p>As can be seen, while some distinction can be drawn between suspicious and benign inputs, there is no clear-cut separation that can yield a highly accurate model.</p>\n<p>Before processing the data further, we perform one-hot encoding on the categorical features as well as split the dataset into training and testing data (80% split).</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df.join(pd.get_dummies(df.loc[:, categorical_columns]))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df.drop(categorical_columns, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train, X_test, y_train, y_test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> train_test_split(df.drop([</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), df[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">test_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">random_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<p>The shape of the training data becomes 20153 x 118. One-hot encoding significantly increased the number of features we were concerned with: we now have 118 features to train a model on. Considering that most features weren’t highly correlated to begin with, we decide to perform feature engineering on our dataset before training any model on it.</p>\n<h2 id=\"feature-engineering-pca\">Feature Engineering (PCA)</h2>\n<p>We first begin by scaling our data to the standard normal distribution (Gaussian with mean 0 and standard deviation 1). Note that we fit the distribution on the training data and scale the test data accordingly, to prevent data leakage from the training to the testing set.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">scaler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> preprocessing.StandardScaler()  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scaler.fit(X_train)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scaler.transform(X_train)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_test_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scaler.transform(X_test)</span></span>\n<span class=\"line\"></span></code></pre>\n<p>After doing so, we perform Principal Component Analysis (PCA) which reduces the dimensionality of our data. It is a form of unsupervised algorithm that produces a small set of uncorrelated variables called principal components. Its benefit is manifold including (1) reducing the complexity of our models, (2) helping alleviate the “curse of dimensionality”, and (3) increasing the interpretability of the data. We decide to perform PCA to obtain 10 resulting components, i.e., new feature columns to use for our models.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">pca </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PCA(</span><span style=\"color:#FFAB70\">n_components</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train_PCA </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pca.fit_transform(X_train_norm)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_test_PCA </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pca.transform(X_test_norm)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">explained_variance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pca.explained_variance_ratio_  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">explained_variance</span></span>\n<span class=\"line\"></span></code></pre>\n<p>The explained variance ratio of those 10 components are:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">0.0834838</span><span style=\"color:#E1E4E8\"> , </span><span style=\"color:#79B8FF\">0.05251576</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.03534561</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.03201952</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.02585129</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\"> 0.02270666</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01908112</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01513291</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01363081</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01235519</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span></code></pre>\n<p>Note that the components are sorted by this score. After plotting the Pearson correlation matrix for these new features as well as the class label, we obtain this new heat map:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df_PCA </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pd.DataFrame(X_train_PCA).corr()  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df_PCA[</span><span style=\"color:#9ECBFF\">'class'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_train  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sns.heatmap(df_PCA.corr(), </span><span style=\"color:#FFAB70\">annot</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/pca_heatmap.png\" alt=\"Correlation heatmap of the 10 principal components with the class label\"></p>\n<p>It is interesting to see that the correlation of the features among each other is -0.11 (which is close to -1/9) since PCA produces uncorrelated features with equal correlation score. Concerning their correlation with the class label (last column or last index), we can note that most of these components are more strongly correlated with the label that their original 42 features. In fact, only one of them has a value of 0.071 while the others have values greater than or equal to 0.35 in absolute value. Compare that with the previous heatmap we calculated on the original features where most of them had a score close to 0.01 in absolute value.</p>\n<p>In order to appreciate the power of PCA, we also perform a scatter plot using the first two PCA components as well as a 3D scatter plot using the first three components. This results in the following two figures:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], X_train_PCA))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], X_train_PCA))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">z </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], X_train_PCA))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_train  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">colors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> matplotlib.colors.ListedColormap([</span><span style=\"color:#9ECBFF\">'green'</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#9ECBFF\">'red'</span><span style=\"color:#E1E4E8\">])  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.scatter(x,y, </span><span style=\"color:#FFAB70\">c</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">labels, </span><span style=\"color:#FFAB70\">cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">colors, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">s</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">40</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.xlabel(</span><span style=\"color:#9ECBFF\">\"First PCA Component\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.ylabel(</span><span style=\"color:#9ECBFF\">\"Second PCA Component\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/pca_2d.png\" alt=\"Scatter plot of the 1st and 2nd principal components\"></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">fig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">14</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">14</span><span style=\"color:#E1E4E8\">))  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> plt.axes(</span><span style=\"color:#FFAB70\">projection</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'3d'</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">labels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_train  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Creating plot  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scatter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ax.scatter3D(x, y, z, </span><span style=\"color:#FFAB70\">c</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">labels, </span><span style=\"color:#FFAB70\">cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">colors)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.set_zlim(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.set_xlim(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.set_xlabel(</span><span style=\"color:#9ECBFF\">\"First PCA Component\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.set_ylabel(</span><span style=\"color:#9ECBFF\">\"Second PCA Component\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.set_zlabel(</span><span style=\"color:#9ECBFF\">\"Third PCA Component\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">legend1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ax.legend(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">scatter.legend_elements(),  </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    loc</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"upper right\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">title</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Classes\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ax.add_artist(legend1)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.show()</span></span>\n<span class=\"line\"></span></code></pre>\n<p><img src=\"/src/assets/ml_nid/pca_3d.png\" alt=\"3D Scatter plot of the 1st, 2nd, and 3rd principal components\"></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The power of feature dimensionality reduction through PCA allowed us to easily separate the class labels across the first two or three principal components. Surprisingly, the PCA algorithm was able to do that without looking at the class labels! The algorithm works only on the input dataset of features, not the labels. Feature engineering was able to significantly simplify the problem of anomaly detection by using the newly produced 10 principal components instead of the original 118 features.</p>";

				const frontmatter = {"title":"Machine Learning for Network Intrusion Detection — Part I: Feature Engineering","description":"In this project, we will explore the area of developing a machine learning model for network intrusion detection. This is a two-part project. The first part focuses on data pre-processing and feature engineering. The second part trains and evaluates different machine learning models.","pubDate":"Dec 17 2023","heroImage":"src/assets/ml_nid/network.jpg","tags":["Data Analysis","Data Visualization","Feature Engineering","Python","Scikit-learn","Matplotlib"]};
				const file = "/Users/elirizk/Desktop/Personal Website/public_site/src/content/projects/ML-NIDS-1.md";
				const url = undefined;
				function rawContent() {
					return "\nTraditional intrusion detection systems have used the notion of allow or block lists to only permit harmless operations or to detect known malicious activities, respectively. This requires a regularly updated configuration file listing the user operations to allow or block, e.g. known malicious IP addresses, outside worktime logins, etc. While this type of network intrusion detection system requires constant modification and a considerable initial investment to produce the original configuration, a novel approach to intrusion detection has emerged: the use of machine learning models to automate the task of detecting potential intruders. As such, in this post, we will develop a machine learning model to recognize hidden patterns in anomalous network activity. To this end, we will perform some data pre-processing on the dataset we obtain, explore various machine learning models, and recommend the best model according to their performance and other metrics. The dataset used for the rest of our implementation can be found [here](https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection).\n\n## Import libraries and set display configuration\n\nBefore processing our dataset, we will have to import the necessary libraries to use their helpful commands.\n\n```python\nimport torch  \nfrom torch.utils.data import DataLoader, TensorDataset  \nfrom torchviz import make_dot  \nimport pandas as pd  \nimport numpy as np  \nimport matplotlib  \nimport matplotlib.pyplot as plt  \nfrom mpl_toolkits.mplot3d import Axes3D  \nfrom sklearn.model_selection import train_test_split  \nfrom sklearn import metrics, preprocessing  \nfrom sklearn.decomposition import PCA  \nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.tree import DecisionTreeClassifier, plot_tree  \nfrom sklearn.ensemble import RandomForestClassifier  \nfrom tqdm import tqdm  \nimport seaborn as sns\n```\n\nI also like setting the display configuration of the pandas module to render all columns, rows, or sequences (as opposed to setting a threshold on its maximum value). This can be done with the following code:\n\n```python\npd.set_option('display.max_columns', None)  \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_seq_item', None)\n```\n\n## Exploratory Data Analysis\n\nWe will start by examining the columns available in the dataset.\n\n```python\ndf.columns  \ndf = pd.read_csv('Train_data.csv')  \ndf.columns\n```\n\nWe obtain the following list of features and class labels (note that the column _class_ contains the label of the entry: Normal or Anomaly). We also note that the columns: _protocol_type_, _service_ and _flag_ are categorical while the rest are numerical. The total number of columns for this dataset is 43 columns.\n\n```python\n['duration', 'protocol_type', 'service', 'flag', 'src_bytes',  \n'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',  \n'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',  \n'su_attempted', 'num_root', 'num_file_creations', 'num_shells',  \n'num_access_files', 'num_outbound_cmds', 'is_host_login',  \n'is_guest_login', 'count', 'srv_count', 'serror_rate',  \n'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',  \n'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',  \n'dst_host_srv_count', 'dst_host_same_srv_rate',  \n'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',  \n'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',  \n'dst_host_srv_serror_rate', 'dst_host_rerror_rate',  \n'dst_host_srv_rerror_rate', 'class']\n```\n\nTo check for class imbalances, we plot the number of entries that are labelled as suspicious versus normal. This can safeguard against potential biases in the model being trained (adjusting for class imbalances).\n\n```python\nsns.countplot(x=df['class'], palette=['green', 'red'], hue=df['class'])\n```\n!['Class Count](/src/assets/ml_nid/class_count.png)\nAs we can see both classes are almost equally represented in the dataset. Hence, we don’t need to remove any instances of the majority class (undersampling).\n\n## Data Correlation\n\nWe now go ahead with transforming the class label column to a binary value (0 for normal and 1 for anomaly) and providing the Pearson correlation matrix of the dataset (excluding the categorical feature columns).\n\n```python\ncategorical_columns = ['protocol_type', 'service', 'flag']  \ndf['class'] = df['class'].apply(lambda x: 0 if x==\"normal\" else 1)  \nplt.figure(figsize=(40,30))  \nsns.heatmap(df.drop(categorical_columns, axis=1).corr(), annot=True)\n``` \n![Correlation matrix with the 39 numerical features and the class label (last column/row)](/src/assets/ml_nid/corr_heatmap.png)\n\nAs can be seen, most features aren’t highly correlated together (colored in light purple for a value close to 0) while some feature pairs have either high positive (close to 1) or negative (close to -1) correlation (colored in white or dark purple respectively). For the class label column (last column and last index), only 9 of the 42 numerical features are highly correlated (with an absolute value greater than 0.5). However, this doesn’t mean that we can easily disregard the features with correlation close to 0. This is because the Pearson correlation value assumes a linear relationship between the two variable, which is an overly simplistic assumption in our case (it might be the case that the interplay of different features is highly correlated with the class label or that the feature is highly correlated in a nonlinear fashion). For example, we decided to plot the variation of the same server rate variable against the destination host name server rate, producing the following scatter plot. Note that for the rest of this post, entries labeled as normal are visualized in green while those labeled anomalous are labeled in red.\n\n```python\nlabels = df['class']  \ncolors = ['green','red']  \ny = df['dst_host_same_srv_rate']  \nx = df['same_srv_rate']  \nplt.scatter(x,y, c=labels, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.5, s=40)  \nplt.xlabel(\"Same Server Rate\")  \nplt.ylabel(\"Destination Host Name Server Rate\")\n```\n![Scatter plot of the same server rate versus the destination host name server rate](/src/assets/ml_nid/same_srv_rate_vs_dst_host_same_srv_rate.png)\n\nAs can be seen, while some distinction can be drawn between suspicious and benign inputs, there is no clear-cut separation that can yield a highly accurate model.\n\nBefore processing the data further, we perform one-hot encoding on the categorical features as well as split the dataset into training and testing data (80% split).\n\n```python\ndf = df.join(pd.get_dummies(df.loc[:, categorical_columns]))  \ndf = df.drop(categorical_columns, axis=1)  \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['class'], axis=1), df['class'], test_size=0.2, random_state=42)\n```\n\nThe shape of the training data becomes 20153 x 118. One-hot encoding significantly increased the number of features we were concerned with: we now have 118 features to train a model on. Considering that most features weren’t highly correlated to begin with, we decide to perform feature engineering on our dataset before training any model on it.\n\n## Feature Engineering (PCA)\n\nWe first begin by scaling our data to the standard normal distribution (Gaussian with mean 0 and standard deviation 1). Note that we fit the distribution on the training data and scale the test data accordingly, to prevent data leakage from the training to the testing set.\n\n```python\nscaler = preprocessing.StandardScaler()  \nscaler.fit(X_train)  \nX_train_norm = scaler.transform(X_train)  \nX_test_norm = scaler.transform(X_test)\n```\n\nAfter doing so, we perform Principal Component Analysis (PCA) which reduces the dimensionality of our data. It is a form of unsupervised algorithm that produces a small set of uncorrelated variables called principal components. Its benefit is manifold including (1) reducing the complexity of our models, (2) helping alleviate the “curse of dimensionality”, and (3) increasing the interpretability of the data. We decide to perform PCA to obtain 10 resulting components, i.e., new feature columns to use for our models.\n\n```python\npca = PCA(n_components = 10)  \n  \nX_train_PCA = pca.fit_transform(X_train_norm)  \nX_test_PCA = pca.transform(X_test_norm)  \n  \nexplained_variance = pca.explained_variance_ratio_  \nexplained_variance\n```\n\nThe explained variance ratio of those 10 components are:\n\n```python\n[0.0834838 , 0.05251576, 0.03534561, 0.03201952, 0.02585129,\n 0.02270666, 0.01908112, 0.01513291, 0.01363081, 0.01235519]\n```\n\nNote that the components are sorted by this score. After plotting the Pearson correlation matrix for these new features as well as the class label, we obtain this new heat map:\n\n```python\ndf_PCA = pd.DataFrame(X_train_PCA).corr()  \ndf_PCA['class'] = y_train  \nsns.heatmap(df_PCA.corr(), annot=True)\n```\n![Correlation heatmap of the 10 principal components with the class label](/src/assets/ml_nid/pca_heatmap.png)\n\nIt is interesting to see that the correlation of the features among each other is -0.11 (which is close to -1/9) since PCA produces uncorrelated features with equal correlation score. Concerning their correlation with the class label (last column or last index), we can note that most of these components are more strongly correlated with the label that their original 42 features. In fact, only one of them has a value of 0.071 while the others have values greater than or equal to 0.35 in absolute value. Compare that with the previous heatmap we calculated on the original features where most of them had a score close to 0.01 in absolute value.\n\nIn order to appreciate the power of PCA, we also perform a scatter plot using the first two PCA components as well as a 3D scatter plot using the first three components. This results in the following two figures:\n\n```python\nx = list(map(lambda x: x[0], X_train_PCA))  \ny = list(map(lambda x: x[1], X_train_PCA))  \nz = list(map(lambda x: x[2], X_train_PCA))  \nlabels = y_train  \ncolors = matplotlib.colors.ListedColormap(['green','red'])  \nplt.scatter(x,y, c=labels, cmap=colors, alpha=0.2, s=40)  \nplt.xlabel(\"First PCA Component\")  \nplt.ylabel(\"Second PCA Component\")\n```\n![Scatter plot of the 1st and 2nd principal components](/src/assets/ml_nid/pca_2d.png)\n```python\nfig = plt.figure(figsize=(14,14))  \nax = plt.axes(projection='3d')  \nlabels = y_train  \n  \n# Creating plot  \nscatter = ax.scatter3D(x, y, z, c=labels, cmap=colors)  \nax.set_zlim(-4,2)  \nax.set_xlim(-4,3)  \nax.set_xlabel(\"First PCA Component\")  \nax.set_ylabel(\"Second PCA Component\")  \nax.set_zlabel(\"Third PCA Component\")  \nlegend1 = ax.legend(*scatter.legend_elements(),  \n                    loc=\"upper right\", title=\"Classes\")  \nax.add_artist(legend1)  \nplt.show()\n```\n![3D Scatter plot of the 1st, 2nd, and 3rd principal components](/src/assets/ml_nid/pca_3d.png)\n## Conclusion\n\nThe power of feature dimensionality reduction through PCA allowed us to easily separate the class labels across the first two or three principal components. Surprisingly, the PCA algorithm was able to do that without looking at the class labels! The algorithm works only on the input dataset of features, not the labels. Feature engineering was able to significantly simplify the problem of anomaly detection by using the newly produced 10 principal components instead of the original 118 features.";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [{"depth":2,"slug":"import-libraries-and-set-display-configuration","text":"Import libraries and set display configuration"},{"depth":2,"slug":"exploratory-data-analysis","text":"Exploratory Data Analysis"},{"depth":2,"slug":"data-correlation","text":"Data Correlation"},{"depth":2,"slug":"feature-engineering-pca","text":"Feature Engineering (PCA)"},{"depth":2,"slug":"conclusion","text":"Conclusion"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${maybeRenderHead()}${unescapeHTML(html)}`;
				});

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, rawContent, url };
